{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitts</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@robbiebronniman Sounds like a great night.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Damn the person who stolde my wallet !!!!!  Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Greetings from the piano bench  (photo) http:/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@drewryanscott i love it!! i love you!! haha f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kissthestars Pretty pretty pretty please, pak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Really upset  http://myloc.me/5x1T</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@lilyroseallen big pool or paddling pool?! mig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@arianna_skye  Hee!   I did tweet...  And you'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is happy to have Tickets for the Concerts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@mileycyrus http://twitpic.com/78urd - Miley, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>must.go.to.bed.early!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shhhh...Quitely Now...Come See... I found a HO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>uh oh.. the sound is messed up on cable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Festï¿½s on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What? I'm not the best?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>k finally made to the hammock and this is the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@mitchelmusso Not allowed to call as I live in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@sandysaikman I do! It sucks! Makes me sad  an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My favourite is Silver Hook which is very nutt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FINALLY leaving hell  my car sounds awful...ugh.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               twitts  sentiment\n",
       "0        @robbiebronniman Sounds like a great night.           1\n",
       "1   Damn the person who stolde my wallet !!!!!  Ma...          1\n",
       "2   Greetings from the piano bench  (photo) http:/...          1\n",
       "3   @drewryanscott i love it!! i love you!! haha f...          1\n",
       "4   @kissthestars Pretty pretty pretty please, pak...          0\n",
       "5                  Really upset  http://myloc.me/5x1T          0\n",
       "6   @lilyroseallen big pool or paddling pool?! mig...          1\n",
       "7   @arianna_skye  Hee!   I did tweet...  And you'...          1\n",
       "8          is happy to have Tickets for the Concerts           1\n",
       "9   @mileycyrus http://twitpic.com/78urd - Miley, ...          1\n",
       "10                             must.go.to.bed.early!           0\n",
       "11  Shhhh...Quitely Now...Come See... I found a HO...          1\n",
       "12           uh oh.. the sound is messed up on cable           0\n",
       "13                                   The Festï¿½s on           1\n",
       "14                           What? I'm not the best?           0\n",
       "15  k finally made to the hammock and this is the ...          1\n",
       "16  @mitchelmusso Not allowed to call as I live in...          0\n",
       "17  @sandysaikman I do! It sucks! Makes me sad  an...          0\n",
       "18  My favourite is Silver Hook which is very nutt...          0\n",
       "19   FINALLY leaving hell  my car sounds awful...ugh.          0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/raw data/twitt30k.csv\")\n",
    "df.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p3\">Preprocess Text</a>\n",
    "**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n",
    "\n",
    "**The Preprocessing steps taken are:**\n",
    "1. **Lower Casing:** Each text is converted to lowercase.\n",
    "2. **Replacing URLs:** Links starting with **\"http\" or \"https\" or \"www\"** are replaced by **\"URL\"**.\n",
    "3. **Replacing Emojis:** Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. *(eg: \":)\" to \"EMOJIsmile\")*\n",
    "4. **Replacing Usernames:** Replace @Usernames with word **\"USER\"**. *(eg: \"@Nilay\" to \"USER\")*\n",
    "5. **Removing Non-Alphabets:** Replacing characters except Digits and Alphabets with a space.\n",
    "6. **Removing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. *(eg: \"Heyyyy\" to \"Heyy\")*\n",
    "7. **Removing Short Words:** Words with length less than 2 are removed.\n",
    "8. **Removing Stopwords:** Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. *(eg: \"the\", \"he\", \"have\")*\n",
    "9. **Lemmatizing:** Lemmatization is the process of converting a word to its base form. *(e.g: “Great” to “Good”)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw=stopwords.words('english')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegEx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'e', 'y', 'o', 'u', 'a', 'l', 'l']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1=\"See you all IN 2023\"\n",
    "re.findall(\"[a-z]\",s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'I', 'N']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z]\",s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '0', '2', '3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[0-9]\",s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See you all IN ****'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[0-9]\",\"*\",s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list=list(df[\"twitts\"])\n",
    "sentiment_list=list(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "    wnl=WordNetLemmatizer()\n",
    "\n",
    "    #Define RegEx\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    #patterns for 3 or more consecutive letters by 2 letter.\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\"\n",
    "    #contraction to expansion\n",
    "    #removal of html tags\n",
    "    processed_tweet=[]\n",
    "    for tweet in tweets:\n",
    "        temp_tweet=[]\n",
    "        for word in tweet.split():\n",
    "            if word not in sw:\n",
    "                word=wnl.lemmatize(word)\n",
    "                temp_tweet.append(word)\n",
    "        \n",
    "        tweet=\" \".join(temp_tweet)\n",
    "\n",
    "\n",
    "        tweet=tweet.lower()\n",
    "        tweet=re.sub(urlPattern,\" URL\",tweet)\n",
    "        tweet=re.sub(userPattern,\" USER\",tweet)\n",
    "        tweet=re.sub(alphaPattern,\" \",tweet)\n",
    "        tweet=re.sub(sequencePattern,seqReplacePattern,tweet)\n",
    "\n",
    "\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, emojis[emoji])\n",
    "        \n",
    "        processed_tweet.append(tweet)\n",
    "    return processed_tweet       \n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text=preprocess(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitts</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@robbiebronniman Sounds like a great night.</td>\n",
       "      <td>1</td>\n",
       "      <td>USER sounds like great night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Damn the person who stolde my wallet !!!!!  Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>damn person stolde wallet may karma come back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Greetings from the piano bench  (photo) http:/...</td>\n",
       "      <td>1</td>\n",
       "      <td>greetings piano bench  photo URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@drewryanscott i love it!! i love you!! haha f...</td>\n",
       "      <td>1</td>\n",
       "      <td>USER love it love you haha forget hug you giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kissthestars Pretty pretty pretty please, pak...</td>\n",
       "      <td>0</td>\n",
       "      <td>USER pretty pretty pretty please  pakidownloa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              twitts  sentiment  \\\n",
       "0       @robbiebronniman Sounds like a great night.           1   \n",
       "1  Damn the person who stolde my wallet !!!!!  Ma...          1   \n",
       "2  Greetings from the piano bench  (photo) http:/...          1   \n",
       "3  @drewryanscott i love it!! i love you!! haha f...          1   \n",
       "4  @kissthestars Pretty pretty pretty please, pak...          0   \n",
       "\n",
       "                                    processed_tweets  \n",
       "0                      USER sounds like great night   \n",
       "1  damn person stolde wallet may karma come back ...  \n",
       "2                   greetings piano bench  photo URL  \n",
       "3   USER love it love you haha forget hug you giv...  \n",
       "4   USER pretty pretty pretty please  pakidownloa...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed_tweets\"]=preprocessed_text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"processed_tweets\"],df[\"sentiment\"], \n",
    "                                                    test_size = 0.20, random_state = 0)\n",
    "print(f'Data Split done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "tfidf.fit(X_train)\n",
    "X_train=tfidf.transform(X_train)\n",
    "X_test=tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24000x21357 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 190672 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def build_model(models):\n",
    "    trained_models=[]\n",
    "    for model in models:\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict values for Test dataset\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Print the evaluation metrics for the dataset.\n",
    "        print(model, accuracy_score(y_test,y_pred))\n",
    "\n",
    "        trained_models.append(model)\n",
    "    return trained_models\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models=[RandomForestClassifier(n_estimators=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=10) 0.711\n"
     ]
    }
   ],
   "source": [
    "trained_models=build_model(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('../models/tfidf.pkl','wb')\n",
    "pickle.dump(tfidf, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../models/lr_model.pkl','wb')\n",
    "pickle.dump(trained_models[1], file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../models/rf_model.pkl','wb')\n",
    "pickle.dump(trained_models[0], file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6000x21357 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 45122 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x21357 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11943fc8efd977551329c86a39c0244629d72f9ce60f46a22abca5f74dd30faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
