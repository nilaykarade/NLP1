{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification using LSTM and Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train and test sentence classification using LSTM, and Pretrained Word2Vec.  \n",
    "You can find visualization of our code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most benefits from Pretrained Word Embedding is<br>even unseen words during traning can be predicted well, since pretrained word embedding already trained with larger data set than your train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, below example also can be predicted well, even \"this\", \"best\", \"show\" were not in the train data.<br> Since \"this\" is similar to \"it\", \"best\" is similar to \"good\" and \"show\" is similar to \"movie\" in pretrained word embedding vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LSTM, so we can generate sentence vector with sequence of word embedding.<br>LSTM is advanced RNN which is powerful on long sequence input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\nilay\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\nilay\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.21.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\nilay\\anaconda3\\lib\\site-packages (from tensorflow_hub) (3.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: C:\\Users\\nilay\\AppData\\Local\\Temp\\tfhub_modules\\22bffebf9723377261c6cd19440f9601d0ec68d6\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4316\\2833757421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load Pretrained Word2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://tfhub.dev/google/Wiki-words-250/2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[0;32m    105\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    826\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m   saved_model_proto, debug_info = (\n\u001b[1;32m--> 933\u001b[1;33m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m   \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m   \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m   debug_info_path = file_io.join(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    113\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Cannot parse file {path_to_pbtxt}: {str(e)}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     raise IOError(\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;34mf\"SavedModel file does not exist at: {export_dir}{os.path.sep}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;34mf\"{{{constants.SAVED_MODEL_FILENAME_PBTXT}|\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: C:\\Users\\nilay\\AppData\\Local\\Temp\\tfhub_modules\\22bffebf9723377261c6cd19440f9601d0ec68d6\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Word2Vec\n",
    "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")\n",
    "https://tfhub.dev/google/Wiki-words-250/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed([\"jump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(df):\n",
    "    \"\"\"\n",
    "    get max token counts from train data, \n",
    "    so we use this number as fixed length input to RNN cell\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for row in df['review']:\n",
    "        if len(row.split(\" \")) > max_length:\n",
    "            max_length = len(row.split(\" \"))\n",
    "    return max_length\n",
    "\n",
    "def get_word2vec_enc(reviews):\n",
    "    \"\"\"\n",
    "    get word2vec value for each word in sentence.\n",
    "    concatenate word in numpy array, so we can use it as RNN input\n",
    "    \"\"\"\n",
    "    encoded_reviews = []\n",
    "    for review in reviews:\n",
    "        tokens = review.split(\" \")\n",
    "        word2vec_embedding = embed(tokens)\n",
    "        encoded_reviews.append(word2vec_embedding)\n",
    "    return encoded_reviews\n",
    "        \n",
    "def get_padded_encoded_reviews(encoded_reviews):\n",
    "    \"\"\"\n",
    "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
    "    \"\"\"\n",
    "    padded_reviews_encoding = []\n",
    "    for enc_review in encoded_reviews:\n",
    "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
    "        pad = np.zeros((1, 250))\n",
    "        for i in range(zero_padding_cnt):\n",
    "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
    "        padded_reviews_encoding.append(enc_review)\n",
    "    return padded_reviews_encoding\n",
    "\n",
    "def sentiment_encode(sentiment):\n",
    "    \"\"\"\n",
    "    return one hot encoding for Y value\n",
    "    \"\"\"\n",
    "    if sentiment == 'positive':\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]\n",
    "    \n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    encode text value to numeric value\n",
    "    \"\"\"\n",
    "    # encode words into word2vec\n",
    "    reviews = df['review'].tolist()\n",
    "    \n",
    "    encoded_reviews = get_word2vec_enc(reviews)\n",
    "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
    "    # encoded sentiment\n",
    "    sentiments = df['sentiment'].tolist()\n",
    "    encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n",
    "    X = np.array(padded_encoded_reviews)\n",
    "    Y = np.array(encoded_sentiment)\n",
    "    return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (encode text to number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews_train = [\n",
    "         {'review': 'this is the best movie', 'sentiment': 'positive'},\n",
    "         {'review': 'i recommend you watch this movie', 'sentiment': 'positive'},\n",
    "         {'review': 'it was waste of money and time', 'sentiment': 'negative'},\n",
    "         {'review': 'the worst movie ever', 'sentiment': 'negative'}\n",
    "    ]\n",
    "df = pd.DataFrame(movie_reviews_train)\n",
    "\n",
    "# max_length is used for max sequence of input\n",
    "max_length = get_max_length(df)\n",
    "\n",
    "train_X, train_Y = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6949 - accuracy: 0.7500\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6850 - accuracy: 0.7500\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6752 - accuracy: 0.7500\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6655 - accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6557 - accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6457 - accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6356 - accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6252 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6145 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6034 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5919 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5801 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5677 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5549 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5415 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5277 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5133 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4983 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4828 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4668 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4502 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4332 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4158 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3980 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3799 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3616 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3430 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3244 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3058 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2871 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2686 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2503 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2322 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2144 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1969 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1798 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1632 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1472 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1319 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1174 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1037 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0911 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0690 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0596 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0513 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0440 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0278 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23bd2f9b100>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(train_X, train_Y,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 32)                36224     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,290\n",
      "Trainable params: 36,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "your model can predict correctly even for unseen words from training.  \n",
    "This is the most benefit of using pretrained word embedding.  \n",
    "Why? pretrained embedding will encode [better], [nice] to similar vector of [best]  \n",
    "even if these words were not in train.  \n",
    "therefore, the input vector to RNN is similar, so correct answers for even these unseen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.1724 - accuracy: 1.0000 - 419ms/epoch - 419ms/step\n",
      "Test score: 0.17236685752868652\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "movie_reviews_train = [\n",
    "         {'review': 'this is the best movie', 'sentiment': 'positive'},\n",
    "         {'review': 'i recommend you watch this movie', 'sentiment': 'positive'},\n",
    "         {'review': 'it was waste of money and time', 'sentiment': 'negative'},\n",
    "         {'review': 'the worst movie ever', 'sentiment': 'negative'}\n",
    "    ]\n",
    "\"\"\"\n",
    "movie_reviews_test = [\n",
    "         {'review': 'it is better movie', 'sentiment': 'positive'},\n",
    "         {'review': 'i suggest you see this movie', 'sentiment': 'positive'},\n",
    "         {'review': 'it was just throwing 20 dollars away', 'sentiment': 'negative'},\n",
    "         {'review': 'worse than any show', 'sentiment': 'negative'},\n",
    "         {'review': 'nice movie, so love it', 'sentiment': 'positive'},\n",
    "         {'review': 'terrible', 'sentiment': 'negative'}\n",
    "    ]\n",
    "test_df = pd.DataFrame(movie_reviews_test)\n",
    "\n",
    "test_X, test_Y = preprocess(test_df)\n",
    "\n",
    "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
